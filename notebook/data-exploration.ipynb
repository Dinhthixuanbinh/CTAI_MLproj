{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport os\nimport numpy as np\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchaudio\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report\nimport copy\n","metadata":{"id":"9mHSIaY3e6wo","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:26:30.827159Z","iopub.execute_input":"2025-08-21T09:26:30.827435Z","iopub.status.idle":"2025-08-21T09:26:38.889594Z","shell.execute_reply.started":"2025-08-21T09:26:30.827412Z","shell.execute_reply":"2025-08-21T09:26:38.888838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Download dataset\n\n!gdown 1CdjCD2amHDsjJFfb5OuTbIfwEikgn6u-\n!unzip cremad.zip","metadata":{"id":"qcZnST06e5uT","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:26:38.891417Z","iopub.execute_input":"2025-08-21T09:26:38.891741Z","iopub.status.idle":"2025-08-21T09:27:06.711482Z","shell.execute_reply.started":"2025-08-21T09:26:38.891722Z","shell.execute_reply":"2025-08-21T09:27:06.710574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !git clone https://github.com/baovin/ML_MERC.git","metadata":{"id":"utkGtbL-wuZx","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T03:41:19.423085Z","iopub.execute_input":"2025-08-25T03:41:19.423272Z","iopub.status.idle":"2025-08-25T03:41:20.159497Z","shell.execute_reply.started":"2025-08-25T03:41:19.423256Z","shell.execute_reply":"2025-08-25T03:41:20.158164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #@title Dataloader\n\n# class CramedDataset(Dataset):\n\n#     def __init__(self, args, data, train=True):\n#         self.args = args\n#         self.image = []\n#         self.audio = []\n#         self.label = []\n#         self.train = train\n#         self.audio_length = 256\n\n#         class_dict = {'NEU':0, 'HAP':1, 'SAD':2, 'FEA':3, 'DIS':4, 'ANG':5}\n\n#         self.visual_feature_path = args.visual_path\n#         self.audio_feature_path = args.audio_path\n\n#         for item in data:\n#             audio_path = os.path.join(self.audio_feature_path, item[0] + '.wav')\n#             visual_path = os.path.join(self.visual_feature_path, 'Image-{:02d}-FPS'.format(self.args.fps), item[0])\n\n#             if os.path.exists(audio_path) and os.path.exists(visual_path):\n#                 self.image.append(visual_path)\n#                 self.audio.append(audio_path)\n#                 self.label.append(class_dict[item[1]])\n#             else:\n#                 continue\n\n\n#     def __len__(self):\n#         return len(self.image)\n\n#     def __getitem__(self, idx):\n\n#         # audio\n\n#         waveform, sr = torchaudio.load(self.audio[idx])\n#         fbank = torchaudio.compliance.kaldi.fbank(\n#                 waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n#                 window_type='hanning', num_mel_bins=128, dither=0.0, frame_shift=10)\n\n#         n_frames = fbank.shape[0]\n\n#         p = self.audio_length - n_frames\n#         if p > 0:\n#             m = torch.nn.ZeroPad2d((0, 0, 0, p))\n#             fbank = m(fbank)\n#         elif p < 0:\n#             fbank = fbank[0:self.audio_length, :]\n\n#         fbank = fbank.unsqueeze(0)\n\n#         if self.train:\n#             transform = transforms.Compose([\n#                 transforms.RandomResizedCrop(224),\n#                 transforms.RandomHorizontalFlip(),\n#                 transforms.ToTensor(),\n#                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n#             ])\n#         else:\n#             transform = transforms.Compose([\n#                 transforms.Resize(size=(224, 224)),\n#                 transforms.ToTensor(),\n#                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n#             ])\n\n#         # Visual\n#         image_samples = os.listdir(self.image[idx])\n#         select_index = np.random.choice(len(image_samples), size=self.args.fps, replace=False)\n#         select_index.sort()\n\n#         img = Image.open(os.path.join(self.image[idx], image_samples[select_index[0]])).convert('RGB')\n#         img = transform(img)\n\n#         # label\n#         label = self.label[idx]\n\n#         return fbank, img, label\n\n# def load_cremad(args):\n#     train_csv = '/kaggle/working/ML_MERC/data/CREMAD/train.csv'\n#     test_csv = '/kaggle/working/ML_MERC/data/CREMAD/train.csv'\n\n#     train_df = pd.read_csv(train_csv, header=None)\n#     train, dev = train_test_split(train_df, test_size=0.1)\n\n#     test = pd.read_csv(test_csv, header=None)\n\n#     train_dataset = CramedDataset(args, train.to_numpy(), True)\n#     dev_dataset = CramedDataset(args, dev.to_numpy(), False)\n#     test_dataset = CramedDataset(args, test.to_numpy(), False)\n\n#     return train_dataset, dev_dataset, test_dataset","metadata":{"id":"N-kLwcxbedES","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:27:07.468344Z","iopub.execute_input":"2025-08-21T09:27:07.468585Z","iopub.status.idle":"2025-08-21T09:27:07.480396Z","shell.execute_reply.started":"2025-08-21T09:27:07.468562Z","shell.execute_reply":"2025-08-21T09:27:07.479842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import argparse\n# args = argparse.Namespace(dataset='CREMAD',\n#           modulation='OGM_GE',\n#           fusion_method='concat',\n#           fps=1, audio_path='/kaggle/working/cremad/AudioWAV',\n#           visual_path='/kaggle/working/cremad/',\n#           batch_size88, epochs=10, optimizer='adam',\n#           learning_rate=0.0002, lr_decay_step=70, lr_decay_ratio=0.1,\n#           ckpt_path='/kaggle/working/ML_MERC/ckpt',\n#           train=True, use_tensorboard=False,\n#           tensorboard_path=None, random_seed=0)","metadata":{"id":"8bRrGIOfhIRO","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:27:07.481189Z","iopub.execute_input":"2025-08-21T09:27:07.481753Z","iopub.status.idle":"2025-08-21T09:27:07.493679Z","shell.execute_reply.started":"2025-08-21T09:27:07.481734Z","shell.execute_reply":"2025-08-21T09:27:07.493118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_dataset, dev_dataset, test_dataset = load_cremad(args)\n\n# train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size,\n#                             shuffle=True, pin_memory=True)\n\n# dev_dataloader = DataLoader(dev_dataset, batch_size=args.batch_size,\n#                           shuffle=False, pin_memory=True)\n\n# test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size,\n#                             shuffle=False, pin_memory=True)\n\n# print('Train: {}, Dev: {}, Test: {}'.format(len(train_dataloader), len(dev_dataloader), len(test_dataloader)))","metadata":{"id":"Um05R5JMisAX","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:27:07.494317Z","iopub.execute_input":"2025-08-21T09:27:07.494497Z","iopub.status.idle":"2025-08-21T09:27:07.678390Z","shell.execute_reply.started":"2025-08-21T09:27:07.494482Z","shell.execute_reply":"2025-08-21T09:27:07.677813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(len(train_dataloader))","metadata":{"id":"atQAJV-K15Sq","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:27:07.680776Z","iopub.execute_input":"2025-08-21T09:27:07.681220Z","iopub.status.idle":"2025-08-21T09:27:07.684377Z","shell.execute_reply.started":"2025-08-21T09:27:07.681200Z","shell.execute_reply":"2025-08-21T09:27:07.683804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"D-2Klbe12RVh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# spec, img, label = train_dataset[0]\n# print(spec.shape)\n# print(img.shape)\n# import matplotlib.pyplot as plt\n# plt.imshow(spec[0,:,:].squeeze())\n\n","metadata":{"id":"vEsXxk2b1zxA","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:27:07.685010Z","iopub.execute_input":"2025-08-21T09:27:07.685224Z","iopub.status.idle":"2025-08-21T09:27:08.074761Z","shell.execute_reply.started":"2025-08-21T09:27:07.685206Z","shell.execute_reply":"2025-08-21T09:27:08.074041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # số lượng mẫu muốn hiển thị\n# n = 5\n\n# plt.figure(figsize=(12, 2*n))\n# for i in range(n):\n#     spec, img, label = train_dataset[i]\n\n#     # vẽ spectrogram\n#     plt.subplot(n, 2, 2*i+1)\n#     plt.imshow(spec[0, :, :].squeeze(), aspect=\"auto\", cmap=\"viridis\")\n#     plt.title(f\"Spec #{i} | Label: {label}\")\n#     plt.axis(\"off\")\n\n#     # vẽ ảnh gốc\n#     plt.subplot(n, 2, 2*i+2)\n#     if img.shape[0] == 3:   # ảnh RGB (C,H,W)\n#         plt.imshow(img.permute(1, 2, 0))\n#     else:                   # ảnh grayscale\n#         plt.imshow(img[0, :, :], cmap=\"gray\")\n#     plt.title(f\"Image #{i} | Label: {label}\")\n#     plt.axis(\"off\")\n\n# plt.tight_layout()\n# plt.show()\n","metadata":{"id":"rVR_QJzclqcQ","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:27:08.075549Z","iopub.execute_input":"2025-08-21T09:27:08.075753Z","execution_failed":"2025-08-21T13:53:18.236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #@title Visual model\n# import matplotlib.pyplot as plt\n\n# class BasicConv(nn.Module):\n#     def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n#         super(BasicConv, self).__init__()\n#         self.out_channels = out_planes\n#         self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n#         self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n#         self.relu = nn.ReLU() if relu else None\n\n#     def forward(self, x):\n#         x = self.conv(x)\n#         if self.bn is not None:\n#             x = self.bn(x)\n#         if self.relu is not None:\n#             x = self.relu(x)\n#         return x\n\n\n# class Flatten(nn.Module):\n#     def forward(self, x):\n#         return x.view(x.size(0), -1)\n\n\n# class ChannelGate(nn.Module):\n#     def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n#         super(ChannelGate, self).__init__()\n#         self.gate_channels = gate_channels\n#         self.mlp = nn.Sequential(Flatten(),\n#                                  nn.Linear(gate_channels, gate_channels // reduction_ratio),\n#                                  nn.ReLU(),\n#                                  nn.Linear(gate_channels // reduction_ratio, gate_channels))\n#         self.pool_types = pool_types\n\n#     def forward(self, x):\n#         channel_att_sum = None\n#         for pool_type in self.pool_types:\n#             if pool_type == 'avg':\n#                 avg_pool = F.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n#                 channel_att_raw = self.mlp(avg_pool )\n#             elif pool_type == 'max':\n#                 max_pool = F.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n#                 channel_att_raw = self.mlp(max_pool)\n#             if channel_att_sum is None:\n#                 channel_att_sum = channel_att_raw\n#             else:\n#                 channel_att_sum = channel_att_sum + channel_att_raw\n\n#         scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n#         return x * scale\n\n\n# class ChannelPool(nn.Module):\n#     def forward(self, x):\n#         return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)\n\n\n# class SpatialGate(nn.Module):\n#     def __init__(self):\n#         super(SpatialGate, self).__init__()\n#         kernel_size = 7\n#         self.compress = ChannelPool()\n#         self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n\n#     def forward(self, x):\n#         x_compress = self.compress(x)\n#         x_out = self.spatial(x_compress)\n#         scale = torch.sigmoid(x_out)\n#         return x * scale\n\n\n# class CBAM(nn.Module):\n#     def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n#         super(CBAM, self).__init__()\n#         self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n#         self.SpatialGate = SpatialGate()\n\n#     def forward(self, x):\n#         x_out = self.ChannelGate(x)\n#         x_out = self.SpatialGate(x_out)\n\n#         return x_out\n\n# def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n#     \"\"\"3x3 convolution with padding\"\"\"\n#     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n#                      padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\n# def conv1x1(in_planes, out_planes, stride=1):\n#     \"\"\"1x1 convolution\"\"\"\n#     return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\n# class BasicBlock(nn.Module):\n#     __constants__ = ['downsample']\n\n#     def __init__(self, inplanes, planes, stride=1, downsample=None):\n#         super(BasicBlock, self).__init__()\n#         norm_layer = nn.BatchNorm2d\n#         self.conv1 = conv3x3(inplanes, planes, stride)\n#         self.bn1 = norm_layer(planes)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.conv2 = conv3x3(planes, planes)\n#         self.bn2 = norm_layer(planes)\n#         self.downsample = downsample\n#         self.stride = stride\n\n#     def forward(self, x):\n#         identity = x\n\n#         out = self.conv1(x)\n#         out = self.bn1(out)\n#         out = self.relu(out)\n#         out = self.conv2(out)\n#         out = self.bn2(out)\n\n#         if self.downsample is not None:\n#             identity = self.downsample(x)\n\n#         out += identity\n#         out = self.relu(out)\n\n#         return out\n\n\n# class MulScaleBlock(nn.Module):\n#     __constants__ = ['downsample']\n\n#     def __init__(self, inplanes, planes, stride=1, downsample=None):\n#         super(MulScaleBlock, self).__init__()\n#         norm_layer = nn.BatchNorm2d\n#         scale_width = int(planes / 4)\n\n#         self.scale_width = scale_width\n\n#         self.conv1 = conv3x3(inplanes, planes, stride)\n#         self.bn1 = norm_layer(planes)\n#         self.relu = nn.ReLU(inplace=False)\n\n#         self.conv1_2_1 = conv3x3(scale_width, scale_width)\n#         self.bn1_2_1 = norm_layer(scale_width)\n#         self.conv1_2_2 = conv3x3(scale_width, scale_width)\n#         self.bn1_2_2 = norm_layer(scale_width)\n#         self.conv1_2_3 = conv3x3(scale_width, scale_width)\n#         self.bn1_2_3 = norm_layer(scale_width)\n#         self.conv1_2_4 = conv3x3(scale_width, scale_width)\n#         self.bn1_2_4 = norm_layer(scale_width)\n\n#         self.conv2_2_1 = conv3x3(scale_width, scale_width)\n#         self.bn2_2_1 = norm_layer(scale_width)\n#         self.conv2_2_2 = conv3x3(scale_width, scale_width)\n#         self.bn2_2_2 = norm_layer(scale_width)\n#         self.conv2_2_3 = conv3x3(scale_width, scale_width)\n#         self.bn2_2_3 = norm_layer(scale_width)\n#         self.conv2_2_4 = conv3x3(scale_width, scale_width)\n#         self.bn2_2_4 = norm_layer(scale_width)\n\n#         self.downsample = downsample\n#         self.stride = stride\n\n#     def forward(self, x):\n#         identity = x\n\n#         out = self.conv1(x)\n#         out = self.bn1(out)\n#         out = self.relu(out)\n\n#         sp_x = torch.split(out, self.scale_width, 1)\n\n#         ##########################################################\n#         out_1_1 = self.conv1_2_1(sp_x[0])\n#         out_1_1 = self.bn1_2_1(out_1_1)\n#         out_1_1_relu = self.relu(out_1_1)\n#         out_1_2 = self.conv1_2_2(out_1_1_relu + sp_x[1])\n#         out_1_2 = self.bn1_2_2(out_1_2)\n#         out_1_2_relu = self.relu(out_1_2)\n#         out_1_3 = self.conv1_2_3(out_1_2_relu + sp_x[2])\n#         out_1_3 = self.bn1_2_3(out_1_3)\n#         out_1_3_relu = self.relu(out_1_3)\n#         out_1_4 = self.conv1_2_4(out_1_3_relu + sp_x[3])\n#         out_1_4 = self.bn1_2_4(out_1_4)\n#         output_1 = torch.cat([out_1_1, out_1_2, out_1_3, out_1_4], dim=1)\n\n#         out_2_1 = self.conv2_2_1(sp_x[0])\n#         out_2_1 = self.bn2_2_1(out_2_1)\n#         out_2_1_relu = self.relu(out_2_1)\n#         out_2_2 = self.conv2_2_2(out_2_1_relu + sp_x[1])\n#         out_2_2 = self.bn2_2_2(out_2_2)\n#         out_2_2_relu = self.relu(out_2_2)\n#         out_2_3 = self.conv2_2_3(out_2_2_relu + sp_x[2])\n#         out_2_3 = self.bn2_2_3(out_2_3)\n#         out_2_3_relu = self.relu(out_2_3)\n#         out_2_4 = self.conv2_2_4(out_2_3_relu + sp_x[3])\n#         out_2_4 = self.bn2_2_4(out_2_4)\n#         output_2 = torch.cat([out_2_1, out_2_2, out_2_3, out_2_4], dim=1)\n\n#         out = output_1 + output_2\n\n#         if self.downsample is not None:\n#             identity = self.downsample(x)\n\n#         out += identity\n#         out = self.relu(out)\n\n#         return out\n\n\n# class AttentionBlock(nn.Module):\n#     __constants__ = ['downsample']\n\n#     def __init__(self, inplanes, planes, stride=1, downsample=None):\n#         super(AttentionBlock, self).__init__()\n#         norm_layer = nn.BatchNorm2d\n#         self.conv1 = conv3x3(inplanes, planes, stride)\n#         self.bn1 = norm_layer(planes)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.conv2 = conv3x3(planes, planes)\n#         self.bn2 = norm_layer(planes)\n#         self.downsample = downsample\n#         self.stride = stride\n#         self.cbam = CBAM(planes, 16)\n\n#     def forward(self, x):\n#         identity = x\n\n#         out = self.conv1(x)\n#         out = self.bn1(out)\n#         out = self.relu(out)\n\n#         out = self.conv2(out)\n#         out = self.bn2(out)\n\n#         out = self.cbam(out)\n\n#         if self.downsample is not None:\n#             identity = self.downsample(x)\n\n#         out += identity\n#         out = self.relu(out)\n\n#         return out\n\n\n# class MANet(nn.Module):\n\n#     def __init__(self, block_b, block_m, block_a, layers, num_classes=12666):\n#         super(MANet, self).__init__()\n#         norm_layer = nn.BatchNorm2d\n#         self._norm_layer = norm_layer\n#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n#         self.bn1 = norm_layer(64)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n#         self.layer1 = self._make_layer(block_b, 64, 64, layers[0])\n#         self.layer2 = self._make_layer(block_b, 64, 128, layers[1], stride=2)\n\n#         # In this branch, each BasicBlock replaced by AttentiveBlock.\n#         self.layer3_1_p1 = self._make_layer(block_a, 128, 256, layers[2], stride=2)\n#         self.layer4_1_p1 = self._make_layer(block_a, 256, 512, layers[3], stride=1)\n\n#         self.layer3_1_p2 = self._make_layer(block_a, 128, 256, layers[2], stride=2)\n#         self.layer4_1_p2 = self._make_layer(block_a, 256, 512, layers[3], stride=1)\n\n#         self.layer3_1_p3 = self._make_layer(block_a, 128, 256, layers[2], stride=2)\n#         self.layer4_1_p3 = self._make_layer(block_a, 256, 512, layers[3], stride=1)\n\n#         self.layer3_1_p4 = self._make_layer(block_a, 128, 256, layers[2], stride=2)\n#         self.layer4_1_p4 = self._make_layer(block_a, 256, 512, layers[3], stride=1)\n\n#         # In this branch, each BasicBlock replaced by MulScaleBlock.\n#         self.layer3_2 = self._make_layer(block_m, 128, 256, layers[2], stride=2)\n#         self.layer4_2 = self._make_layer(block_m, 256, 512, layers[3], stride=2)\n\n#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n#         self.fc_1 = nn.Linear(512, num_classes)\n#         self.fc_2 = nn.Linear(512, num_classes)\n\n#         for m in self.modules():\n#             if isinstance(m, nn.Conv2d):\n#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n#             elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n#                 nn.init.constant_(m.weight, 1)\n#                 nn.init.constant_(m.bias, 0)\n\n#     def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n#         norm_layer = self._norm_layer\n#         downsample = None\n#         if stride != 1 or inplanes != planes:\n#             downsample = nn.Sequential(conv1x1(inplanes, planes, stride), norm_layer(planes))\n#         layers = []\n#         layers.append(block(inplanes, planes, stride, downsample))\n#         inplanes = planes\n#         for _ in range(1, blocks):\n#             layers.append(block(inplanes, planes))\n#         return nn.Sequential(*layers)\n\n#     def _forward_impl(self, x, return_embedding=False):\n\n#         x = x.squeeze(dim=0)\n\n#         x = self.conv1(x)\n#         x = self.bn1(x)\n#         x = self.relu(x)\n#         x = self.maxpool(x)\n\n#         x = self.layer1(x)\n#         x = self.layer2(x)\n\n#         # branch 1 ############################################\n#         patch_11 = x[:, :, 0:14, 0:14]\n#         patch_12 = x[:, :, 0:14, 14:28]\n#         patch_21 = x[:, :, 14:28, 0:14]\n#         patch_22 = x[:, :, 14:28, 14:28]\n\n#         branch_1_p1_out = self.layer3_1_p1(patch_11)\n#         branch_1_p1_out = self.layer4_1_p1(branch_1_p1_out)\n\n#         branch_1_p2_out = self.layer3_1_p2(patch_12)\n#         branch_1_p2_out = self.layer4_1_p2(branch_1_p2_out)\n\n#         branch_1_p3_out = self.layer3_1_p3(patch_21)\n#         branch_1_p3_out = self.layer4_1_p3(branch_1_p3_out)\n\n#         branch_1_p4_out = self.layer3_1_p4(patch_22)\n#         branch_1_p4_out = self.layer4_1_p4(branch_1_p4_out)\n\n#         branch_1_out_1 = torch.cat([branch_1_p1_out, branch_1_p2_out], dim=3)\n#         branch_1_out_2 = torch.cat([branch_1_p3_out, branch_1_p4_out], dim=3)\n#         branch_1_out = torch.cat([branch_1_out_1, branch_1_out_2], dim=2)\n\n#         branch_1_out = self.avgpool(branch_1_out)\n#         branch_1_out_embedding = torch.flatten(branch_1_out, 1)\n#         branch_1_out = self.fc_1(branch_1_out_embedding)\n\n#         # branch 2 ############################################\n#         branch_2_out = self.layer3_2(x)\n#         branch_2_out = self.layer4_2(branch_2_out)\n#         branch_2_out = self.avgpool(branch_2_out)\n#         branch_2_out_embedding = torch.flatten(branch_2_out, 1)\n#         branch_2_out = self.fc_2(branch_2_out_embedding)\n\n#         if return_embedding:\n#             return torch.cat([branch_1_out_embedding, branch_2_out_embedding], dim=1)\n#         else:\n#             return branch_1_out, branch_2_out\n\n#     def forward(self, x, return_embedding=False):\n#         return self._forward_impl(x, return_embedding)\n\n\n# def manet(**kwargs):\n#     return MANet(block_b=BasicBlock, block_m=MulScaleBlock, block_a=AttentionBlock, layers=[2, 2, 2, 2], **kwargs)\n\n# class RecorderMeter(object):\n#     \"\"\"Computes and stores the minimum loss value and its epoch index\"\"\"\n\n#     def __init__(self, total_epoch):\n#         self.reset(total_epoch)\n\n#     def reset(self, total_epoch):\n#         self.total_epoch = total_epoch\n#         self.current_epoch = 0\n#         self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)    # [epoch, train/val]\n#         self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n\n#     def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n#         self.epoch_losses[idx, 0] = train_loss * 30\n#         self.epoch_losses[idx, 1] = val_loss * 30\n#         self.epoch_accuracy[idx, 0] = train_acc\n#         self.epoch_accuracy[idx, 1] = val_acc\n#         self.current_epoch = idx + 1\n\n#     def plot_curve(self, save_path):\n\n#         title = 'the accuracy/loss curve of train/val'\n#         dpi = 80\n#         width, height = 1800, 800\n#         legend_fontsize = 10\n#         figsize = width / float(dpi), height / float(dpi)\n\n#         fig = plt.figure(figsize=figsize)\n#         x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n#         y_axis = np.zeros(self.total_epoch)\n\n#         plt.xlim(0, self.total_epoch)\n#         plt.ylim(0, 100)\n#         interval_y = 5\n#         interval_x = 5\n#         plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n#         plt.yticks(np.arange(0, 100 + interval_y, interval_y))\n#         plt.grid()\n#         plt.title(title, fontsize=20)\n#         plt.xlabel('the training epoch', fontsize=16)\n#         plt.ylabel('accuracy', fontsize=16)\n\n#         y_axis[:] = self.epoch_accuracy[:, 0]\n#         plt.plot(x_axis, y_axis, color='g', linestyle='-', label='train-accuracy', lw=2)\n#         plt.legend(loc=4, fontsize=legend_fontsize)\n\n#         y_axis[:] = self.epoch_accuracy[:, 1]\n#         plt.plot(x_axis, y_axis, color='y', linestyle='-', label='valid-accuracy', lw=2)\n#         plt.legend(loc=4, fontsize=legend_fontsize)\n\n#         y_axis[:] = self.epoch_losses[:, 0]\n#         plt.plot(x_axis, y_axis, color='g', linestyle=':', label='train-loss-x30', lw=2)\n#         plt.legend(loc=4, fontsize=legend_fontsize)\n\n#         y_axis[:] = self.epoch_losses[:, 1]\n#         plt.plot(x_axis, y_axis, color='y', linestyle=':', label='valid-loss-x30', lw=2)\n#         plt.legend(loc=4, fontsize=legend_fontsize)\n\n#         if save_path is not None:\n#             fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n#             print('Saved figure')\n#         plt.close(fig)","metadata":{"id":"BeAp7faym3Hs","cellView":"form","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #@title Audio model\n\n# from einops import rearrange, repeat\n# from einops.layers.torch import Rearrange\n\n\n# # helpers\n# def pair(t):\n#     return t if isinstance(t, tuple) else (t, t)\n\n\n# # classes\n# class PreNorm(nn.Module):\n#     def __init__(self, dim, fn):\n#         super().__init__()\n#         self.norm = nn.LayerNorm(dim)\n#         self.fn = fn\n\n#     def forward(self, x, **kwargs):\n#         return self.fn(self.norm(x), **kwargs)\n\n\n# class FeedForward(nn.Module):\n#     def __init__(self, dim, hidden_dim, dropout=0.):\n#         super().__init__()\n#         self.net = nn.Sequential(\n#             nn.Linear(dim, hidden_dim),\n#             nn.GELU(),\n#             nn.Dropout(dropout),\n#             nn.Linear(hidden_dim, dim),\n#             nn.Dropout(dropout)\n#         )\n\n#     def forward(self, x):\n#         return self.net(x)\n\n\n# class Attention(nn.Module):\n#     def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n#         super().__init__()\n#         inner_dim = dim_head * heads\n#         project_out = not (heads == 1 and dim_head == dim)\n\n#         self.heads = heads\n#         self.scale = dim_head ** -0.5\n\n#         self.attend = nn.Softmax(dim=-1)\n#         self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n\n#         self.to_out = nn.Sequential(\n#             nn.Linear(inner_dim, dim),\n#             nn.Dropout(dropout)\n#         ) if project_out else nn.Identity()\n\n#     def forward(self, x):\n#         qkv = self.to_qkv(x).chunk(3, dim=-1)\n#         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n\n#         dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n#         attn = self.attend(dots)\n\n#         out = torch.matmul(attn, v)\n#         out = rearrange(out, 'b h n d -> b n (h d)')\n#         return self.to_out(out)\n\n\n# class Transformer(nn.Module):\n#     def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n#         super().__init__()\n#         self.layers = nn.ModuleList([])\n#         for _ in range(depth):\n#             self.layers.append(nn.ModuleList([\n#                 PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n#                 PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n#             ]))\n\n#     def forward(self, x):\n#         for attn, ff in self.layers:\n#             x = attn(x) + x\n#             x = ff(x) + x\n#         return x\n\n\n# class Scale(nn.Module):\n#     def __init__(self, val):\n#         super().__init__()\n#         self.val = val\n\n#     def forward(self, x):\n#         return x * self.val\n\n\n# class SepTrBlock(nn.Module):\n#     def __init__(self, channels, input_size, heads=3, mlp_dim=128, dim_head=32,\n#                  down_sample_input=None, project=False, reconstruct=False, dim=128, dropout_tr=0.0):\n#         super().__init__()\n#         patch_height, patch_width = pair(input_size)\n#         self.avg_pool = nn.Identity()\n#         self.upsample = nn.Identity()\n#         self.projection = nn.Identity()\n#         self.reconstruction = nn.Identity()\n\n#         if down_sample_input is not None:\n#             patch_height = patch_height // down_sample_input[0]\n#             patch_width = patch_width // down_sample_input[1]\n\n#             self.avg_pool = nn.AvgPool2d(kernel_size=down_sample_input)\n#             self.upsample = nn.UpsamplingNearest2d(scale_factor=down_sample_input)\n\n#         if project:\n#             self.projection = nn.Linear(channels, dim)\n#         if reconstruct:\n#             self.reconstruction = nn.Sequential(\n#                 nn.Linear(dim, channels),\n#                 Scale(dim ** -0.5)\n#             )\n\n#         self.rearrange_patches_h = Rearrange('b c h w -> b w h c')\n#         self.rearrange_patches_w = Rearrange('b c h w -> b h w c')\n\n#         self.rearrange_in_tr = Rearrange('b c h w -> (b c) h w')\n#         self.rearrange_out_tr_h = Rearrange('(b c) h w -> b w h c', c=patch_width)\n#         self.rearrange_out_tr_w = Rearrange('(b c) h w -> b w c h', c=patch_height)\n\n#         self.pos_embedding_w = nn.Parameter(torch.randn(1, 1, patch_width + 1, dim))\n#         self.pos_embedding_h = nn.Parameter(torch.randn(1, 1, patch_height + 1, dim))\n#         self.transformer_w = Transformer(dim, 1, heads, dim_head, mlp_dim, dropout_tr)\n#         self.transformer_h = Transformer(dim, 1, heads, dim_head, mlp_dim, dropout_tr)\n\n#     def forward(self, x, cls_token):\n#         x = self.avg_pool(x)\n#         x = x.squeeze(1)\n\n#         # H inference\n#         h = self.rearrange_patches_h(x)\n#         h = self.projection(h)\n\n#         dim1, dim2, _, _ = h.shape\n#         if cls_token.shape[0] == 1:\n#             cls_token = repeat(cls_token, '() () n d -> b w n d', b=dim1, w=dim2)\n#         else:\n#             cls_token = repeat(cls_token, 'b () n d -> b w n d', w=dim2)\n\n#         h = torch.cat((cls_token, h), dim=2)\n#         h += self.pos_embedding_h\n\n#         h = self.rearrange_in_tr(h)\n#         h = self.transformer_h(h)\n#         h = self.rearrange_out_tr_h(h)\n\n#         # W inference\n#         w = self.rearrange_patches_w(h[:, :, 1:, :])\n\n#         cls_token = h[:, :, 0, :].unsqueeze(2)\n#         cls_token = repeat(cls_token.mean((-1, -2)).unsqueeze(1).unsqueeze(1), 'b () d2 e -> b d1 d2 e', d1=w.shape[1])\n\n#         w = torch.cat((cls_token, w), dim=2)\n#         w += self.pos_embedding_w\n\n#         w = self.rearrange_in_tr(w)\n#         w = self.transformer_w(w)\n#         w = self.rearrange_out_tr_w(w)\n\n#         x = self.upsample(w[:, :, :, 1:])\n#         x = self.reconstruction(x)\n\n#         cls_token = w[:, :, :, 0].mean(2).unsqueeze(1).unsqueeze(1)\n#         return x, cls_token\n\n# class SeparableTr(nn.Module):\n#     def __init__(self, channels=1, input_size=(128, 128), num_classes=35, depth=3, heads=5, mlp_dim=256, dim_head=256,\n#                  down_sample_input=None, dim=256):\n#         super().__init__()\n#         inner_channels = channels\n\n#         self.transformer = nn.ModuleList()\n\n#         if depth < 1:\n#             raise Exception(\"Depth cannot be smaller than 1!\")\n\n#         self.transformer.append(\n#             SepTrBlock(channels=inner_channels, input_size=input_size, heads=heads, mlp_dim=mlp_dim,\n#                        dim_head=dim_head, down_sample_input=down_sample_input, dim=dim, project=True)\n#         )\n\n#         for i in range(1, depth):\n#             self.transformer.append(\n#                 SepTrBlock(channels=inner_channels, input_size=input_size, heads=heads, mlp_dim=mlp_dim,\n#                            dim_head=dim_head, down_sample_input=down_sample_input, dim=dim, project=False)\n#             )\n\n#         self.cls_token = nn.Parameter(torch.randn(1, 1, 1, dim))\n\n#     def forward(self, x):\n#         x, cls_token = self.transformer[0](x, self.cls_token)\n\n#         for i in range(1, len(self.transformer)):\n#             x, cls_token = self.transformer[i](x, cls_token)\n\n#         cls_token = cls_token[:, 0, 0, :]\n#         x = cls_token\n#         return x","metadata":{"id":"1P_sGf3xnkpy","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #@title Fusion\n\n# class SumFusion(nn.Module):\n#     def __init__(self, input_dim=512, output_dim=100):\n#         super(SumFusion, self).__init__()\n#         self.fc_x = nn.Linear(input_dim, output_dim)\n#         self.fc_y = nn.Linear(input_dim, output_dim)\n\n#     def forward(self, x, y):\n#         output = self.fc_x(x) + self.fc_y(y)\n#         return x, y, output\n\n\n# class ConcatFusion(nn.Module):\n#     def __init__(self, input_dim=1024, output_dim=100):\n#         super(ConcatFusion, self).__init__()\n#         self.fc_out = nn.Linear(input_dim, output_dim)\n\n#     def forward(self, x, y):\n#         output = torch.cat((x, y), dim=1)\n#         output = self.fc_out(output)\n#         return x, y, output\n\n\n# class FiLM(nn.Module):\n#     \"\"\"\n#     FiLM: Visual Reasoning with a General Conditioning Layer,\n#     https://arxiv.org/pdf/1709.07871.pdf.\n#     \"\"\"\n\n#     def __init__(self, input_dim=512, dim=512, output_dim=100, x_film=True):\n#         super(FiLM, self).__init__()\n\n#         self.dim = input_dim\n#         self.fc = nn.Linear(input_dim, 2 * dim)\n#         self.fc_out = nn.Linear(dim, output_dim)\n\n#         self.x_film = x_film\n\n#     def forward(self, x, y):\n\n#         if self.x_film:\n#             film = x\n#             to_be_film = y\n#         else:\n#             film = y\n#             to_be_film = x\n\n#         gamma, beta = torch.split(self.fc(film), self.dim, 1)\n\n#         output = gamma * to_be_film + beta\n#         output = self.fc_out(output)\n\n#         return x, y, output\n\n\n# class GatedFusion(nn.Module):\n#     \"\"\"\n#     Efficient Large-Scale Multi-Modal Classification,\n#     https://arxiv.org/pdf/1802.02892.pdf.\n#     \"\"\"\n\n#     def __init__(self, input_dim=512, dim=512, output_dim=100, x_gate=True):\n#         super(GatedFusion, self).__init__()\n\n#         self.fc_x = nn.Linear(input_dim, dim)\n#         self.fc_y = nn.Linear(input_dim, dim)\n#         self.fc_out = nn.Linear(dim, output_dim)\n\n#         self.x_gate = x_gate  # whether to choose the x to obtain the gate\n\n#         self.sigmoid = nn.Sigmoid()\n\n#     def forward(self, x, y):\n#         out_x = self.fc_x(x)\n#         out_y = self.fc_y(y)\n\n#         if self.x_gate:\n#             gate = self.sigmoid(out_x)\n#             output = self.fc_out(torch.mul(gate, out_y))\n#         else:\n#             gate = self.sigmoid(out_y)\n#             output = self.fc_out(torch.mul(out_x, gate))\n\n#         return out_x, out_y, output\n","metadata":{"cellView":"form","id":"DJ17yzTtojve","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #@title Multimodal model\n\n# class AVC_GB(nn.Module):\n#     def __init__(self, args):\n#         super(AVC_GB, self).__init__()\n\n#         fusion = args.fusion_method\n#         n_classes = 6\n\n\n#         self.fusion_module = ConcatFusion(input_dim=1280, output_dim=n_classes)\n\n\n#         self.audio_net = SeparableTr(1, (256, 128), num_classes=6, depth=2, heads=2, mlp_dim=256)\n\n#         self.visual_net = manet(num_classes=n_classes)\n#     def forward(self, audio, visual):\n#         audio = audio.unsqueeze(dim=0)\n#         visual = visual.unsqueeze(dim=0)\n#         v = self.visual_net(visual, return_embedding=True)\n#         a = self.audio_net(audio)\n#         a, v, out = self.fusion_module(a, v)\n#         return a, v, out","metadata":{"id":"rEPwZ5fIn1ZB","cellView":"form","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = AVC_GB(args)\n# a, v, out = model(spec.unsqueeze(0),img.unsqueeze(0))\n# print(a.shape, v.shape, out.shape)\n","metadata":{"id":"6XuB-Ada3w3s","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import time\n# import torch\n\n# model.eval()\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n# spec, img = spec.to(device), img.to(device)\n\n# for _ in range(10):\n#     with torch.no_grad():\n#         _ = model(spec.unsqueeze(0), img.unsqueeze(0))\n\n# num_runs = 100\n# torch.cuda.synchronize()\n# start = time.time()\n\n# with torch.no_grad():\n#     for _ in range(num_runs):\n#         a, v, out = model(spec.unsqueeze(0), img.unsqueeze(0))\n# torch.cuda.synchronize()\n\n# end = time.time()\n# avg_time = (end - start) / num_runs\n\n# print(f\"Thời gian inference trung bình: {avg_time*1000:.3f} ms/lần\")\n# print(\"Output shapes:\", a.shape, v.shape, out.shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# num_gpus = torch.cuda.device_count()\n# print(f\"Số GPU khả dụng: {num_gpus}\")\n\n# for i in range(num_gpus):\n#     print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class FocalLoss(nn.Module):\n#     def __init__(self, alpha=1, gamma=2, reduction='mean'):\n#         super(FocalLoss, self).__init__()\n#         if isinstance(alpha, (list, torch.Tensor)):\n#             self.alpha = torch.tensor(alpha, dtype=torch.float)\n#         else:\n#             self.alpha = alpha\n#         self.gamma = gamma\n#         self.reduction = reduction\n\n#     def forward(self, inputs, targets):\n#         ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha if isinstance(self.alpha, torch.Tensor) else None)\n#         pt = torch.exp(-ce_loss)\n#         focal_loss = (self.alpha.to(inputs.device) if isinstance(self.alpha, torch.Tensor) else self.alpha) * (1 - pt) ** self.gamma * ce_loss\n\n#         if self.reduction == 'mean':\n#             return focal_loss.mean()\n#         elif self.reduction == 'sum':\n#             return focal_loss.sum()\n#         else:\n#             return focal_loss","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #@title Trainning\n\n# from tqdm import tqdm\n# from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n\n# def train_epoch(args, epoch, model, device, dataloader, optimizer, scheduler, writer=None):\n#     # criterion = nn.CrossEntropyLoss()\n#     criterion = FocalLoss(alpha=1, gamma=2)\n\n#     model.train()\n#     print(\"Start training ... \")\n\n#     _loss = 0\n\n#     for step, (spec, image, label) in (pbar := tqdm(enumerate(dataloader), desc='Epoch: {}: '.format(epoch))):\n#         #pdb.set_trace()\n#         spec = spec.to(device)\n#         image = image.to(device)\n#         label = label.to(device)\n#         optimizer.zero_grad()\n\n#         # TODO: make it simpler and easier to extend\n#         a, v, out = model(spec.float(), image.float())\n\n#         loss = criterion(out, label)\n#         loss.backward()\n\n#         optimizer.step()\n#         # pbar.set_description('Epoch: {} Loss: {:.4f}'.format(epoch, loss.item()))\n\n#         _loss += loss.item()\n\n#     scheduler.step()\n\n#     return _loss / len(dataloader)\n\n# def eval(args, model, device, dataloader, test=False):\n#     softmax = nn.Softmax(dim=1)\n\n#     if args.dataset == 'CREMAD':\n#         n_classes = 6\n#     else:\n#         raise NotImplementedError('Incorrect dataset name {}'.format(args.dataset))\n\n#     with torch.no_grad():\n#         model.eval()\n#         # criterion = nn.CrossEntropyLoss()\n#         criterion = FocalLoss(alpha=1, gamma=2)\n#         _loss = 0\n#         golds = []\n#         preds = []\n#         for step, (spec, image, label) in enumerate(dataloader):\n\n#             spec = spec.to(device)\n#             image = image.to(device)\n#             label = label.to(device)\n\n#             a, v, out = model(spec.float(), image.float())\n\n\n#             loss = criterion(out, label)\n#             _loss += loss.item()\n\n#             y_hat = torch.argmax(softmax(out), dim=-1)\n#             golds.extend(label.cpu().numpy())\n#             preds.extend(y_hat.cpu().numpy())\n\n#         wf1 = f1_score(golds, preds, average='weighted')\n#         acc = accuracy_score(golds, preds)\n#         prec = precision_score(golds, preds, average='weighted', zero_division=0)\n#         rec = recall_score(golds, preds, average='weighted', zero_division=0)\n\n#         if test:\n#             print(classification_report(golds, preds))\n#     return _loss / len(dataloader), wf1, acc, prec, rec","metadata":{"id":"nRQ2g01toDeD","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from torch import optim\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# model = AVC_GB(args)\n\n# if torch.cuda.device_count() > 1:\n#     print(\"Using DataParallel on\", torch.cuda.device_count(), \"GPUs\")\n#     model = torch.nn.DataParallel(model)   \n\n# model.to(device)\n\n# optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=1e-4)\n# scheduler = optim.lr_scheduler.StepLR(optimizer, args.lr_decay_step, args.lr_decay_ratio)","metadata":{"id":"d42Xi1rHpx3N","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if args.train:\n#         best_dev_f1 = 0.0\n#         best_acc  = 0.0\n#         best_state = None\n#         best_epoch = 0\n\n#         for epoch in range(args.epochs):\n#             batch_loss= train_epoch(args, epoch, model, device,\n#                                         train_dataloader, optimizer, scheduler)\n#             loss, f1, acc, prec, rec = eval(args, model, device, dev_dataloader)\n\n#             print(\n#                     \"Epoch: {}, Train Loss: {:.4f}, Dev Loss: {:.4f}, \"\n#                     \"Acc: {:.4f}, Prec: {:.4f}, Rec: {:.4f}, F1: {:.4f}\".format(\n#                         epoch, batch_loss, loss, acc, prec, rec, f1\n#                     )\n#                     )\n\n\n#             # if f1 > best_dev_f1:\n#             #     best_dev_f1 = f1\n#             #     best_state = best_state = copy.deepcopy(model.state_dict())\n#             #     best_epoch = epoch\n#             if acc > best_acc:\n#                 best_acc = acc\n#                 best_state = best_state = copy.deepcopy(model.state_dict())\n#                 best_epoch = epoch\n\n#         # Best state\n#         model.load_state_dict(best_state)\n#         # print('Best model loaded at epoch {} with dev F1: {}'.format(best_epoch, best_dev_f1))\n#         # print(\"Start testing on test dataset ...\")\n#         # _, f1 = eval(args, model, device, test_dataloader, test=True)\n#         # if not os.path.exists(args.ckpt_path):\n#         #     os.mkdir(args.ckpt_path)\n\n#         print('Best model loaded at epoch {} with accuracy: {}'.format(best_epoch, best_acc))\n#         print(\"Start testing on test dataset ...\")\n#         loss, f1, acc, prec, rec = eval(args, model, device, test_dataloader, test=True)\n#         if not os.path.exists(args.ckpt_path):\n#             os.mkdir(args.ckpt_path)\n\n#         # model_name = 'best_model_of_dataset_{}_' \\\n#         #                 'optimizer_{}_' \\\n#         #                 'epoch_{}_f1_{}.pth'.format(args.dataset,\n#         #                                             args.optimizer,\n#         #                                             epoch, f1)\n\n#         # saved_dict = {'saved_epoch': epoch,\n#         #                 'fusion': args.fusion_method,\n#         #                 'f1': f1,\n#         #                 'model': model.state_dict(),\n#         #                 'optimizer': optimizer.state_dict(),\n#         #                 'scheduler': scheduler.state_dict()}\n#         model_name = 'best_model_of_dataset_{}_' \\\n#                         'optimizer_{}_' \\\n#                         'epoch_{}_acc_{}.pth'.format(args.dataset,\n#                                                     args.optimizer,\n#                                                     epoch, acc)\n\n#         saved_dict = {'saved_epoch': epoch,\n#                         'fusion': args.fusion_method,\n#                         'acc': acc,\n#                         'model': model.state_dict(),\n#                         'optimizer': optimizer.state_dict(),\n#                         'scheduler': scheduler.state_dict()}\n\n#         save_dir = os.path.join(args.ckpt_path, model_name)\n\n#         torch.save(saved_dict, save_dir)\n#         print('The best model has been saved at {}.'.format(save_dir))\n\n\n\n# else:\n#         loaded_dict = torch.load(args.ckpt_path)\n#         fusion = loaded_dict['fusion']\n#         state_dict = loaded_dict['model']\n\n#         assert fusion == args.fusion_method, 'inconsistency between fusion method of loaded model and args !'\n\n#         model.load_state_dict(state_dict)\n#         print('Trained model loaded! Testing ...')\n\n#         loss, f1, acc, prec, rec  = eval(args, model, device, test_dataloader, test=True)\n\n\n","metadata":{"id":"SumTcXnj1Aaf","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os, re, torch\n\n# ckpt_dir = \"/kaggle/working/ML_MERC/ckpt\"\n# ckpt_files = [f for f in os.listdir(ckpt_dir) if f.endswith(\".pth\")]\n\n# best_acc = -1\n# best_file = None\n\n# for f in ckpt_files:\n#     match = re.search(r\"acc_(\\d+\\.?\\d*)\", f)\n#     if match:\n#         acc = float(match.group(1))\n#         if acc > best_acc:\n#             best_acc = acc\n#             best_file = f\n\n# if best_file is None:\n#     raise FileNotFoundError(\"Không tìm thấy checkpoint\")\n\n# best_path = os.path.join(ckpt_dir, best_file)\n# print(f\"Loading checkpoint: {best_path} (Acc={best_acc:.4f})\")\n\n# checkpoint = torch.load(best_path, map_location=\"cpu\")\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pre_trained_dict = checkpoint['state_dict']\n# pre_trained_dict = {k.replace('module.', ''): v for k,v in checkpoint['state_dict'].items()}\n\n# model.visual_net.load_state_dict(pre_trained_dict)\n# model.audio_net.load_state_dict(pre_trained_dict)","metadata":{"id":"DAud9j5dWsHC","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Best state\n# model.load_state_dict(best_state)\n# print('Best model loaded at epoch {} with dev acc: {}'.format(best_epoch, best_acc))\n# print(\"Start testing on test dataset ...\")\n# loss, f1, acc, prec, rec = eval(args, model, device, test_dataloader, test=True)","metadata":{"id":"XDh_A-G1T11w","trusted":true,"execution":{"execution_failed":"2025-08-21T13:53:18.252Z"}},"outputs":[],"execution_count":null}]}